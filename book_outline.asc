== Intro

=== Why depth? Why the Kinect?
* Background on the Vision Revolution (see attached essay: "Making Things See")
* Kinect is the most successful computer peripheral in history

=== What is the Kinect? How does it work?
* Teardown: RGB camera, IR projector, IR camera, servos, mics.  http://www.slideshare.net/atduskgreg/body-tracking-with-the-kinect[see my slide deck from Health 2.0 Boston] 
* Explanation of IR dot displacement and how the depth calculation works
* Discussion of PrimeSense, OpenNI, libfreenect, Open Kinect, licensing

=== Who is this book for/expectations
* Expected skills when starting out:
* Required reading:
** Getting Started with Processing or equivalent
** Getting Started with Arduino or basic Arduino skills for robotics section
** Willingness to learn lots of new environments and concepts
        
== Fundamentals

=== Closest pixel
* Explanation of how depth information is stored as a grayscale image
* Introduction to the Processing Kinect library
* Learn how to iterate through the depth image to find and track the closest pixel
* Discussion of alignment issue between color and depth image
* Projects
** Hello world: RGB image side-by-side with depth image
** Draw a line by waving your hand

=== Blob tracking

* What is computer vision? Overview of basic ideas in traditional approaches
* Explanation of why depth image is better than color image for computer vision
* Intro to OpenCV: what is it? Overview of Processing library basics
* Use the depth image along with OpenCV to track hands, user outlines, etc.
* Do background subtraction based on what's not moving in the scene
* Projects:
** Substitute the background with fake sunset image
** Depth data zoetrope
** Addition to hand drawing project from last chapter: use other hand as eraser

=== Point cloud

* What is a "point cloud"? Why would we want one?
* Orientation to 3D space and axes
* From 2D to 3D: translate depth value into z-axis to draw a 3D point cloud
* Intro to basic 3D drawing: cameras, lighting, world axis
* Project
** Kinect stereo viewer

=== Skeletonization

* Explanation of how skeleton and user tracking are extracted from depth image
* Intro to OpenNI API and concepts
* Learn how to access joint position and orientation information from OpenNI
* Use joint position information to calculate vectors to draw bones
* Learn more 3D drawing: vectors, translation, 3D primitives
* Projects
** draw a basic stickman
** invisible accordion: trigger sound based on relative movement of hands


== Gestural Interfaces for Assistive Technology

* Discussion of challenges of assistive technology as an example of gestural interfaces: limited range of motion, problems of scanning interfaces, etc.
* Many of the techniques in this chapter are derived from working with actual patients and their occupational therapists at NYU
* The human body as computer input device: limb location, limb relations, limb motion
** Tracking a single body part via closest pixel
** Tracking a single body part via skeleton tracking
** Discussions of limitations of skeleton tracking for less mobile users
** Generating mouse and keyboard events from within Processing

Next, there is a set of getting started type projects.  *Maybe these are code examples, instead?  Need some way to differentiate between projects and samples?*

* Getting started
** hand wave mouse with clicking
** hand wave handwriting
** Using areas in space as targets for clicking
** Creating a 3D bounding box and checking to see how many points are within it
** Invisible interface

Then, there are other projects listed.  Not sure how these relate?  Are these projects, or discussion points?

* Discussion of how physical disabilities/injuries alter range of motion
* Discussion of exercise as therapy: copying a particular body motion, making sure range of motion is achieved
* Pose recognition
* Normalizing joint motion from a starting pose
* Tracking and evaluating joint motion over time
* Recording and comparing skeleton data
* Tracking multiple users from OpenNI
* Projects
** float hot points to trigger streaming radio
** desk drum kit
** Canabalt-style jumping game

Finally, another set of projects.  *I think this chapter has gotten a bit overly ambitious, maybe?*

* Projects
** triggering songs from a famous dance pose
** Exercise evaluation from therapist demonstration
** something with yoga

== 3D Scanning for Fabrication

* Why rapid prototyping matters for design and fabrication: story of the Glif
* How 3D printing is being used in art
* Introduction to rapid prototyping methods and options: laser cutting, CNC, 3D printing, Makerbot, Shapeways
* We're going to go all the way from depth image of us in front of Kinect to holding a print of ourselves in our hands

* Key concepts
** What can we print? Discussion of attributes of printable geometry: water tightness, etc.
** Building a mesh from the Kinect point cloud: triangles and quads
** Exporting a mesh as an obj or ply file
** Using Meshlab to evaluate and repair mesh
** Discussion of normals and how they determine inside and outside of geometry
* Intro to 3D modeling tools (with Blender? (because it's free) Cinema 4D? (because it's actually easy to use))
** Final clean-up of mesh in Blender/C4D
* Fabrication
** Submitting a file to Shapeways
** Printing a file on the Makerbot
** Discussion of other fabrication options
** Explanation of possibility of color in printing and using the calibrated color image to generate a texture file

* Project
** Self-portrait bust "Frozen in carbonite"


== Motion Tracking for Animation and Games
Discussion of how Kinect brings current special effects techniques within reach of everyone: motion capture, 3D scanning as character input, etc.  Also, use of motion tracking, 3D animation techniques in games.

This combines the last two chapters: scanning and mesh cleanup to get scan, gestural interface to control 3D objects and capture motion.
* Note proliferation of 3D data via Photosynth, Google 3D warehouse, etc.

* Overview of different 3D modeling approaches
** NURBS, polygon, etc. 
** Revisit our 3D modeling program from last chapter
* Key concepts
** Importing a 3D model (load in our scan from last chapter)
** Rigging a model for animation: mapping skeleton data to portions of the mesh
** Applying textures to a model
** Introduction to OpenGL lighting

* Projects:
** Animated Face Puppet
*** Download and clean up a Google warehouse model
*** Intro to physics engines
*** Applying a physics engine to the model
** Be godzilla: import and manipulate a model of the Empire state building from Google 3D Warehouse
*** Recording motion capture data from the skeleton
*** Creating and saving BVH files
*** Importing BVH files into Blender/C4D
** Animate a wolf puppet from your own recording crawling motion

== 3D Vision for Robots
Brief history of robot vision, its aims and trials: from SAIL's Shakey to the Darpa Grand Challenge

=== Introduction to kinematics
=== Calculating angles between limb vectors
=== Servos as analogs of joints
=== Inverse Kinematics --  how would you calculate the servo angles if we only told you the position of the hand?

=== Projects
* servo robot arm
** Obstacle detection with depth data: is there something in front of me?
** Feedback from vision: course charting
* skeleton-chasing robot
    - Building up a picture of the environment
    - Remembering obstacle location
    - Introduction to SLAM: a strategy for aligning multiple scans into a single mesh
* Kinect roomba

== Conclusion
This chapter introduces areas for further study: prompts for projects that are now possible with these skills.  It provides:
* More resources: getting greater depth on each of these fields
* Beyond Processing: working in C++
* The future of depth sensors
